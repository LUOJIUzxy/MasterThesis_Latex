% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Methodology}\label{chapter:methodology}

\section{Data Collection and Preprocessing}

\subsection{Asset Selection and Justification}


In constructing the portfolio for this study, we selected five stocks representing five major sectors out of the eleven sectors defined by the Global Industry Classification Standard (GICS): Information Technology, Health Care, Industrials, Consumer Discretionary, and Financials. These sectors were chosen due to their significant contributions to the S\&P 500 Index, collectively accounting for the majority of the index's market capitalization. The selected stocks are as follows: \ac{MSFT} from the Information Technology sector, \ac{LLY} from the Health Care sector, \ac{ETN} from the Industrials sector, \ac{HLT} from the Consumer Discretionary sector, and \ac{JPM} from the Financials sector. And these five individual stocks are selected based on entropy analysis and comparison.

\paragraph{Stock Market Sectors}
The rationale behind this selection is multifaceted. Firstly, by including stocks from these predominant sectors, the portfolio captures the performance dynamics of the key drivers of the U.S. equity market. This approach ensures that the portfolio is both diversified and representative of broader market movements. Diversification across different sectors helps mitigate unsystematic risk associated with individual industries or companies, allowing the portfolio to benefit from growth opportunities while reducing the impact of sector-specific downturns.

Secondly, focusing on sectors that contribute most significantly to the S\&P 500 enhances the relevance of the study's findings to investors who benchmark their performance against this widely recognized index. The Information Technology sector, for example, encompasses leading companies in innovation and technology, significantly influencing market trends. The Health Care sector includes firms pivotal in advancing medical technology and pharmaceuticals, demonstrating resilience in various economic conditions.

Industrials and Consumer Discretionary sectors represent core components of the economy, reflecting business cycles and consumer spending patterns, respectively. The Financials sector plays a critical role in economic infrastructure, encompassing banking, insurance, and investment services. By selecting assets from these sectors, the portfolio encompasses areas crucial to economic growth and market capitalization.

This strategic selection aligns with the study's objective of developing a predictive portfolio optimization framework applicable to a realistic and impactful set of assets. It allows for testing the effectiveness of the proposed models and strategies in a context that is meaningful to investors and relevant to overall market performance. Moreover, by focusing on sectors that significantly influence the S\&P 500, the study ensures that its results have practical implications for portfolio management and investment decision-making.

\paragraph{Entropy Methods for Time Series Analysis}
To assess the complexity of the time-series data for these assets, we utilized entropy-based methods. Specifically, we employed the \texttt{OrdianlEntropy} package in Python, which provides time-efficient, ordinal pattern-based entropy algorithms for computing the complexity of one-dimensional time-series. This analysis informed our feature engineering process by highlighting the inherent unpredictability and dynamic behavior of the asset prices.

\subparagraph{Permutation Entropy (PE)}
Permutation Entropy (PE), introduced by Bandt and Pompe in 2002, is a measure of complexity that quantifies the diversity of ordinal patterns in a time series. It is based on the relative ordering of the values within embedding vectors derived from the time series.
Given a time series ${ x_t }_{t=1}^N$, the calculation of PE involves the following steps:

\begin{enumerate} \item \textbf{Embedding the Time Series:}
Choose an embedding dimension $m$ and time delay $\tau$, and construct embedding vectors:
\begin{equation}
    s_i = \left( x_i, x_{i+\tau}, x_{i+2\tau}, \dots, x_{i+(m-1)\tau} \right), \quad i = 1,2,\dots, N - (m -1)\tau.
\end{equation}

\item \textbf{Forming Ordinal Patterns:}

For each embedding vector $s_i$, determine the permutation $\pi_i$ that sorts its components in ascending order:
\begin{equation}
    s_i \rightarrow \left( x_{i+\tau(k_0)} \leq x_{i+\tau(k_1)} \leq \dots \leq x_{i+\tau(k_{m-1})} \right),
\end{equation}
where $(k_0, k_1, \dots, k_{m-1})$ represents the indices of the sorted components.

\item \textbf{Calculating Probabilities:}

Count the frequency of each of the $m!$ possible ordinal patterns $\pi_j$, and estimate their probabilities:
\begin{equation}
    p(\pi_j) = \frac{\text{Number of times pattern } \pi_j \text{ occurs}}{N - (m -1)\tau}.
\end{equation}

\item \textbf{Calculating Permutation Entropy:}

Compute the permutation entropy using Shannon's entropy formula:
\begin{equation}
    \mathrm{PE} = - \sum_{j=1}^{m!} p(\pi_j) \ln p(\pi_j).
\end{equation}

Normalize PE by dividing by $\ln(m!)$ to ensure it ranges between 0 and 1:
\begin{equation}
    \mathrm{PE}_{\text{norm}} = \frac{\mathrm{PE}}{\ln(m!)}.
\end{equation}
\end{enumerate}

\begin{itemize} \item A \textbf{low PE} value indicates that the time series has a predictable or regular structure, with fewer unique ordinal patterns. \item A \textbf{high PE} value suggests that the time series is more complex or random, exhibiting a higher diversity of ordinal patterns. \end{itemize}


\subparagraph{Weighted Permutation Entropy (WPE)}
Weighted Permutation Entropy (WPE) enhances PE by incorporating the amplitude information of the time series. It assigns weights to ordinal patterns based on the values within the embedding vectors, thus accounting for both the order and magnitude of the data.
The calculation of WPE involves the following steps:

\begin{enumerate} \item \textbf{Embedding and Ordinal Patterns:}
Follow steps 1 and 2 as in the calculation of PE to obtain the embedding vectors and their corresponding ordinal patterns.

\item \textbf{Assigning Weights:}

For each embedding vector $s_i$, calculate a weight $w_i$ based on the amplitude of its components. A common choice is the variance or absolute differences:
\begin{equation}
    w_i = \operatorname{Var}(s_i) = \frac{1}{m} \sum_{k=0}^{m-1} \left( x_{i+\tau k} - \bar{x}_i \right)^2,
\end{equation}
where $\bar{x}_i$ is the mean of the components of $s_i$.

\item \textbf{Calculating Weighted Probabilities:}

For each ordinal pattern $\pi_j$, compute the weighted probability:
\begin{equation}
    p_w(\pi_j) = \frac{\sum_{s_i \in \pi_j} w_i}{\sum_{i=1}^{N - (m -1)\tau} w_i}.
\end{equation}

\item \textbf{Calculating Weighted Permutation Entropy:}

Compute WPE using the weighted probabilities:
\begin{equation}
    \mathrm{WPE} = - \sum_{j=1}^{m!} p_w(\pi_j) \ln p_w(\pi_j).
\end{equation}

Normalize WPE similarly to PE:
\begin{equation}
    \mathrm{WPE}_{\text{norm}} = \frac{\mathrm{WPE}}{\ln(m!)}.
\end{equation}
\end{enumerate}
\begin{itemize} \item \textbf{WPE} reflects both the diversity of ordinal patterns and the magnitude of fluctuations in the time series. \item It is more sensitive to significant changes in amplitude, making it useful for detecting volatility and abrupt transitions. \end{itemize}

\subparagraph{Dispersion Entropy (DE)}
Dispersion Entropy (DE), introduced by Rostaghi and Azami in 2016, is an entropy measure that accounts for both the ordering and dispersion of data by mapping the time series into a sequence of classes or symbols.
To calculate DE, follow these steps:

\begin{enumerate} \item \textbf{Mapping Data to Classes:}
Define $c$ classes and map each data point $x_t$ to a class label $k_t$ using a mapping function, such as:
\begin{equation}
    k_t = \left\lfloor c \cdot \frac{x_t - x_{\min} + \epsilon}{x_{\max} - x_{\min} + 2\epsilon} \right\rfloor + 1, \quad k_t \in \{1, 2, \dots, c\},
\end{equation}
where $x_{\min}$ and $x_{\max}$ are the minimum and maximum values of the time series, and $\epsilon$ is a small constant to prevent division by zero.

\item \textbf{Embedding:}

Form embedding vectors of class labels:
\begin{equation}
    s_i = \left( k_i, k_{i+\tau}, k_{i+2\tau}, \dots, k_{i+(m-1)\tau} \right).
\end{equation}

\item \textbf{Creating Dispersion Patterns:}

Each embedding vector $s_i$ represents a dispersion pattern $d_j$, where $j$ indexes the possible patterns.

\item \textbf{Calculating Probabilities:}

Count the frequency of each possible dispersion pattern and estimate their probabilities:
\begin{equation}
    p(d_j) = \frac{\text{Number of times pattern } d_j \text{ occurs}}{N - (m -1)\tau}.
\end{equation}

\item \textbf{Calculating Dispersion Entropy:}

Compute DE using:
\begin{equation}
    \mathrm{DE} = - \sum_{j=1}^{c^m} p(d_j) \ln p(d_j).
\end{equation}

Normalize DE by dividing by $\ln(c^m)$:
\begin{equation}
    \mathrm{DE}_{\text{norm}} = \frac{\mathrm{DE}}{\ln(c^m)}.
\end{equation}
\end{enumerate}

\begin{itemize} \item \textbf{DE} considers both the frequency and dispersion of data, providing a robust measure of complexity for non-stationary time series. \item It effectively captures changes in amplitude and is less sensitive to noise and outliers. \end{itemize}

\subparagraph{Choosing WPE and DE for Stock Price Analysis}
Analyzing stock prices requires accounting for both the order and magnitude of price changes due to the following reasons:

\subparagraph{Advantages of WPE}

\begin{itemize} \item \textbf{Amplitude Incorporation:} By weighting ordinal patterns, WPE accounts for the magnitude of price changes, capturing volatility. \item \textbf{Volatility Sensitivity:} It is effective in detecting periods of high market volatility, which are critical for risk management. \item \textbf{Balanced Complexity:} WPE balances computational efficiency with the need for detailed analysis. \end{itemize}

\subparagraph{Advantages of DE}

\begin{itemize} \item \textbf{Non-Stationarity Adaptation:} DE effectively handles non-stationary time series by mapping data into classes. \item \textbf{Noise Robustness:} It is less sensitive to noise and outliers, providing reliable complexity measures in the presence of market anomalies. \item \textbf{Comprehensive Analysis:} DE considers both the frequency and amplitude of patterns, offering a holistic view of market dynamics. \end{itemize}
\begin{itemize} \item \textbf{Amplitude Sensitivity:} The size of price movements affects returns and risk; thus, methods that incorporate amplitude information are essential. \item \textbf{Volatility Detection:} Financial markets are characterized by varying volatility, which influences investment decisions. \item \textbf{Non-Stationarity Handling:} Stock prices exhibit trends and abrupt changes, necessitating entropy measures that handle non-stationary data effectively. \end{itemize}

\subparagraph{Conclusion}
Weighted Permutation Entropy and Dispersion Entropy are more suitable than Permutation Entropy for analyzing stock prices because they:

\begin{itemize} \item Capture essential information about both the direction and magnitude of price movements. \item Are better equipped to handle the inherent non-stationarity and volatility of financial time series. \item Provide more informative and sensitive complexity measures that can enhance asset selection and risk assessment strategies. \end{itemize}

By utilizing WPE and DE, analysts and investors can gain deeper insights into market behavior, aiding in the selection of assets across different sectors based on the complexity and predictability of their stock prices.
\subsection{Data Sources and Time Period}


To acquire the historical market data necessary for this study, we employed the EOD Historical Data API, a reputable and comprehensive source for end-of-day and historical financial data across various asset classes. The data retrieval process was automated using a Python script that constructs API requests based on the asset ticker, desired data period (e.g., daily, weekly), and specified date range.

For U.S.-listed assets, the script utilizes the endpoint format:

\begin{verbatim}
https://eodhd.com/api/eod/{ticker}.US?period={period}&api_token={api_token}
\end{verbatim}

where \texttt{\{ticker\}} represents the stock symbol, \texttt{\{period\}} denotes the data frequency, \texttt{\{api\_token\}} is the authentication token, and \texttt{\{start\_date\}} and \texttt{\{end\_date\}} define the data range. For Bitcoin (BTC), which is categorized differently in the API, the script accesses data using the endpoint:

\begin{verbatim}
https://eodhd.com/api/eod/BTC-USD.CC?period={period}&api_token={api_token}
\end{verbatim}

An API token, securely stored using environment variables to maintain confidentiality, is included in the requests for authentication. The script handles HTTP responses by checking for successful status codes and raising exceptions in case of errors, ensuring robust data retrieval.

Upon receiving a valid response, the JSON data is parsed into a pandas DataFrame for efficient data manipulation and analysis. The DataFrame includes essential financial indicators such as open, high, low, close prices, and trading volumes. The data is then saved as a CSV file in a structured directory hierarchy corresponding to each asset, following the path:

\begin{verbatim}
../Stocks/{ticker}/{ticker}_us_{period}.csv
\end{verbatim}

By automating the data fetching and saving process, we ensured consistency and repeatability in data collection of the whole pipeline. This method allowed us to systematically gather historical market data for all selected assets over the specified time periods, providing a reliable dataset for training the Gaussian Process Regression models and conducting backtesting for the portfolio optimization strategies. The use of the EOD Historical Data API ensured that the data was up-to-date and accurate, reflecting real market conditions essential for the validity of our analysis.

Typically, stocks indexes data like S\&P500 and Nasdaq100 is fetched from https://www.nasdaq.com/, are used to represent the overall market performance. In this study, we included the S\&P 500 index as a benchmark for the U.S. equity market. The S\&P 500 index is widely regarded as a barometer for the U.S. stock market and is composed of 500 large-cap companies representing various sectors.

\subsection{Data Preprocessing and Log Returns}
Data preprocessing and feature engineering are critical steps in preparing the dataset for modeling. They ensure that the data fed into the Gaussian Process Regression models are clean, consistent, and informative.

\paragraph{Use of Log Returns}

In this project, log returns are utilized for modeling asset price movements due to several compelling reasons that align with both theoretical and practical considerations in financial analysis.

\subparagraph{Theoretical Foundation in Finance}

Log returns are integral to many foundational financial models, such as the Black-Scholes option pricing model, which assume that asset prices follow a log-normal distribution. By using log returns, we align our modeling approach with these theoretical frameworks, facilitating more accurate and consistent analyses. \ac{GPR} models assume normally distributed outputs, and since log returns of log-normally distributed prices are normally distributed, this compatibility enhances the effectiveness of our predictive modeling.

\subparagraph{Stability Over Time}

Log returns exhibit greater stability compared to simple arithmetic returns, particularly in the presence of extreme outliers or during periods of high market volatility. They tend to smooth out spikes and reduce the impact of short-term noise, making the models less sensitive to sudden market anomalies. This stability is crucial for developing robust predictive models that can perform reliably under various market conditions.

\subparagraph{Time Consistency (Additivity)}

One of the key mathematical properties of log returns is their additive nature over time. The total log return over a period is the sum of the log returns over sub-periods:

\begin{equation}
\log\left( \frac{S_t}{S_0} \right) = \log\left( \frac{S_t}{S_{t-1}} \right) + \log\left( \frac{S_{t-1}}{S_{t-2}} \right) + \dots + \log\left( \frac{S_1}{S_0} \right),
\end{equation}

where $S_t$ is the asset price at time $t$. This additive property simplifies the computation of returns over arbitrary time horizons, such as weekly or monthly periods, by allowing us to sum daily log returns. It is particularly beneficial for forecasting and portfolio optimization over multi-day horizons, as it facilitates the aggregation of returns without the need for complex compounding calculations.

\subparagraph{Normalization of Price Scale}

Log returns are scale-invariant, meaning they standardize returns across assets regardless of their price levels. Whether an asset is priced at \$1 or \$1,000, the log return brings their percentage changes onto a consistent scale. This normalization simplifies comparisons across assets with vastly different price levels and reduces the need for additional data scaling or normalization procedures. It ensures that no single asset disproportionately influences the model due to its absolute price, allowing for a more balanced and equitable analysis within the portfolio.

\subparagraph{Conclusion}

By incorporating log returns into our modeling framework, we leverage their theoretical compatibility with financial models, enhance stability against market volatility, benefit from their time-additive properties, and achieve scale normalization across diverse assets. These advantages contribute to the robustness and accuracy of our Gaussian Process Regression models and improve the effectiveness of our dynamic portfolio optimization strategies.



\paragraph{Data Normalization and Scaling}

To bring all features onto a similar scale and improve the numerical stability of the models, we applied data normalization techniques. Specifically, we used min-max scaling to normalize the historical return features and the time index:

\begin{equation}
    X_{\text{normalized}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}},
\end{equation}

where $X$ represents the original feature values, and $X_{\text{min}}$ and $X_{\text{max}}$ are the minimum and maximum values of the feature, respectively. This scaling transforms the data to a [0, 1] range, facilitating efficient model training.

\paragraph{Treatment of Missing Data and Outliers}

Financial time-series data often contain missing values and outliers due to market closures, data recording errors, or extreme market events. To address missing data, we employed interpolation methods appropriate for time-series, such as linear interpolation and forward/backward filling, ensuring temporal continuity in the data.

Outliers were identified using the Interquartile Range (IQR) method:

\begin{equation}
    \text{IQR} = Q_3 - Q_1,
\end{equation}

where $Q_1$ and $Q_3$ are the first and third quartiles, respectively. Data points lying outside 1.5 times the IQR from the quartiles were considered outliers. We assessed these outliers to determine whether they were due to data errors or genuine market anomalies. Genuine outliers representing significant market movements were retained to preserve the dataset's integrity, while erroneous data points were corrected or removed.

\paragraph{Data Splitting and Cross-Validation}

The dataset was divided into training and testing sets to evaluate the model's predictive performance. The training set consisted of the first 80\% of the time period, while the remaining 20\% was reserved for testing. This chronological split respects the temporal order of the data, avoiding look-ahead bias.

To further validate the models, we used time-series cross-validation with a rolling window approach. In each iteration, the model was trained on a window of consecutive data points and tested on the subsequent period. This method provides a more robust assessment of the model's performance over time and simulates real-world forecasting conditions.

\paragraph{Sliding Window Approach}

To denoise the time-series data and reduce short-term fluctuations, we employed a sliding window approach using a centered rolling window mechanism. For each data point in the series, a window of a specified size $w$ was centered around it, and a statistical function was applied to the data within this window to compute a denoised value. The primary function used was the mean, though other functions could be applied as needed.

Formally, let $\{ x_t \}_{t=1}^T$ represent the original time-series data, and $\{ \tilde{x}_t \}_{t=1}^T$ denote the denoised series. The denoised value at time $t$, $\tilde{x}_t$, is calculated as:

\begin{equation}
\tilde{x}_t = \frac{1}{n_t} \sum_{i = t - k}^{t + k} x_i,
\end{equation}

where $k = \left\lfloor \frac{w}{2} \right\rfloor$, and $n_t$ is the number of data points within the window centered at time $t$. The window size $w$ is an odd integer to ensure symmetry around the central point. At the edges of the time-series (when $t - k < 1$ or $t + k > T$), the window is adjusted by including available data points, and the minimum number of periods is set to 1 to allow computation even with incomplete windows.

To handle any missing values that may arise at the edges due to insufficient data points, we applied forward and backward filling methods. Forward filling propagates the last observed non-missing value forward to fill subsequent missing positions, while backward filling fills missing values by propagating the next observed non-missing value backward. These steps ensure that the denoised series is complete and free from missing values.

This sliding window denoising process effectively smooths the data by averaging over the local neighborhood of each data point, reducing random noise while preserving significant trends and patterns. By enhancing the signal-to-noise ratio in the time-series, this approach improves the quality of the input data for the Gaussian Process Regression models, leading to better predictive performance and more reliable portfolio optimization decisions.

\paragraph{Gaussian Filter Denoising Method}

To further enhance the quality of the time-series data and reduce high-frequency noise, we employed the Gaussian filter denoising method. The Gaussian filter is a convolutional filter that applies a Gaussian kernel to smooth data by averaging neighboring points with weights determined by the Gaussian function. This technique preserves significant trends and patterns while effectively attenuating random fluctuations and noise.

The Gaussian kernel is defined by the Gaussian (normal) distribution function:

\begin{equation}
G(i) = \frac{1}{\sqrt{2\pi} \sigma} \exp\left( -\frac{i^2}{2\sigma^2} \right),
\end{equation}

where $i$ is the index distance from the central data point, and $\sigma$ is the standard deviation of the Gaussian distribution, controlling the degree of smoothing. A larger $\sigma$ results in a wider kernel and more extensive smoothing.

The denoised value at time $t$, $\tilde{x}_t$, is computed by convolving the original time-series data $x_t$ with the Gaussian kernel:

\begin{equation}
\tilde{x}_t = \sum_{i = -k}^{k} G(i) \cdot x_{t + i},
\end{equation}

where $k$ is the window size parameter determining the range of data points considered around time $t$.

In our implementation, we applied the Gaussian filter to the closing prices of the assets using a standard deviation $\sigma = 1$, which provides a balanced smoothing effect without overly distorting the data. The following code snippet illustrates the application of the Gaussian filter using the \texttt{gaussian\_filter} function from the SciPy library in Python:

\begin{lstlisting}[language=Python]
if isFiltered:
    df['filtered_close'] = gaussian_filter(df['close'], sigma=1)
\end{lstlisting}

In this code, \texttt{df['close']} represents the original closing price data, and \texttt{df['filtered\_close']} stores the denoised data after applying the Gaussian filter. The \texttt{isFiltered} flag allows for conditional application of the filtering process.

By using the Gaussian filter denoising method, we effectively reduced the impact of noise and short-term fluctuations in the financial time-series data. This preprocessing step enhances the signal-to-noise ratio, allowing the Gaussian Process Regression models to focus on underlying market trends and improving the accuracy of the return predictions. The combination of the sliding window approach and Gaussian filtering provides a robust methodology for data smoothing, contributing significantly to the reliability and performance of the predictive modeling and subsequent portfolio optimization.

The filtered data retained essential market movements while reducing random fluctuations, allowing the Gaussian Process Regression models to focus on meaningful signals.


\subsection{Summary}

By carefully selecting assets, sourcing reliable data, and meticulously preprocessing the dataset, we established a solid foundation for our predictive modeling. The combination of normalization, outlier treatment, strategic data splitting, and denoising ensured that the inputs to our models were of high quality. These steps are crucial for enhancing the performance of the Gaussian Process Regression models and, ultimately, for developing effective portfolio optimization strategies.



\section{Model Development}
\subsection{Performance metrics}
MSE, MAE, RMSE, MAPE
Sharpe ratio
\subsection{Baseline models}
ARIMA model
\subsection{Hyperparameter tuning}
\subsection{Model evaluation and comparison}
\subsection{Portfolio optimization strategies}
\subsection{Transaction costs and rebalancing}
\subsection{Strategy selection mechanism}
\subsection{Implementation details}

\section{Forecasting Approach}
Describe the iterative forecasting method and how predictions are updated daily.
\subsection{Multi-input Gaussian Process Regression model}
\subsection{Kernel functions selection and hyperparameter optimization}
\subsection{Implementation of rolling window predictions}
\subsection{Model updating mechanism}

\section{Portfolio Optimization Strategies}
Explain each portfolio optimization strategy in depth, including mathematical formulations and constraints.
\subsection{Traditional Strategies}
Maximum return strategy formulation
Minimum volatility approach
Maximum Sharpe ratio optimization
Constraint specifications and justifications as Baseline models
\subsection{Dynamic Strategy}
Probability distribution modeling

\section{Probability Estimation: $P(S_1 > S_2)$}

When estimating the probability $P(S_1 > S_2)$ for two random variables $S_1$ and $S_2$, the methodology depends on the nature of their distributions and their dependence structure. This section outlines three approaches: numerical integration, Monte Carlo simulation, and the use of copulas for dependent variables.

\subsection{Numerical Integration}

If the probability density functions (PDFs) of $S_1$ and $S_2$ are known, the probability $P(S_1 > S_2)$ can be expressed as:
\begin{equation}
P(S_1 > S_2) = \int_{-\infty}^\infty \int_{y}^\infty f_{S_1}(x) f_{S_2}(y) \, dx \, dy,
\end{equation}
where:
\begin{itemize}
    \item $f_{S_1}(x)$ is the PDF of $S_1$,
    \item $f_{S_2}(y)$ is the PDF of $S_2$.
\end{itemize}

This double integral represents the joint probability over the region where $S_1 > S_2$, and it requires numerical methods for evaluation when closed-form solutions are unavailable.


\subsection{Copulas for Dependence}

Especially, When $S_1$ and $S_2$ are dependent, the joint distribution can be modeled using a copula. A copula is a function that describes the dependence structure between random variables, linking their marginal distributions. Let $F_{S_1}(x)$ and $F_{S_2}(y)$ represent the cumulative distribution functions (CDFs) of $S_1$ and $S_2$, respectively. The joint CDF can be expressed as:
\begin{equation}
F_{S_1, S_2}(x, y) = C(F_{S_1}(x), F_{S_2}(y)),
\end{equation}
where $C(u, v)$ is the copula function.

The probability $P(S_1 > S_2)$ can then be computed as:
\begin{equation}
P(S_1 > S_2) = \int_{-\infty}^\infty \int_{y}^\infty \frac{\partial^2 C(F_{S_1}(x), F_{S_2}(y))}{\partial u \partial v} \, dx \, dy.
\end{equation}

The steps to compute this are:
\begin{enumerate}
    \item Determine the marginal distributions $F_{S_1}(x)$ and $F_{S_2}(y)$ for $S_1$ and $S_2$.
    \item Select an appropriate copula function $C(u, v)$ based on the dependence structure (e.g., Gaussian, Clayton, or Gumbel copulas).
    \item Use numerical methods to evaluate the double integral above.
\end{enumerate}

Copulas are particularly effective when the marginal distributions are non-normal or when the dependence structure is non-linear and cannot be captured by simple correlation measures.

\subsection{Monte Carlo Methods}

If the distributions of $S_1$ and $S_2$ are not explicitly known but sampling from these distributions is possible, a Monte Carlo simulation can be used to estimate $P(S_1 > S_2)$. The steps are as follows:
\begin{enumerate}
    \item Generate $N$ independent samples $S_1^{(i)}$ and $S_2^{(i)}$ from the respective distributions of $S_1$ and $S_2$.
    \item Count the number of instances where $S_1^{(i)} > S_2^{(i)}$. Denote this count by $n$.
    \item Estimate the probability as:
    \begin{equation}
    P(S_1 > S_2) \approx \frac{n}{N},
    \end{equation}
    where
    \begin{equation}
    n = \sum_{i=1}^N \mathbb{I}(S_1^{(i)} > S_2^{(i)}),
    \end{equation}
    and $\mathbb{I}(\cdot)$ is the indicator function, which equals 1 if the condition is true and 0 otherwise.
\end{enumerate}

Monte Carlo simulation is particularly useful for complex distributions or dependent variables, where analytical integration is impractical. In our case, we have multiple assets with potentially non-normal distributions and complex dependencies, making Monte Carlo methods a valuable tool for estimating $P(S_1 > S_2)$.
Specifically, we will use Monte Carlo simulation to estimate the probability of one asset outperforming another in our portfolio optimization strategies. And we chose a sample size of $N = 10,000$ to ensure accurate probability estimates.

\subsection{Comparison of Methods}

\begin{itemize}
    \item \textbf{Numerical Integration:} Provides an exact solution given the PDFs of $S_1$ and $S_2$, but computationally intensive for high-dimensional problems or non-standard distributions.
    \item \textbf{Copulas:} Allows modeling of complex dependence structures, particularly useful for non-normal distributions or asymmetric dependencies.
    \item \textbf{Monte Carlo Simulation:} Flexible and practical alternative when sampling is straightforward, though its accuracy depends on the number of samples $N$.
\end{itemize}

Each method has its strengths and limitations, given the availability of distributional information and computational resources of our case, we chose to use Monte Carlo simulation for estimating $P(S_1 > S_2)$.


\subsection{Strategy switching criteria}
Describe the criteria for switching between portfolio optimization strategies based on the estimated probability $P(S_1 > S_2)$.
We set a threshold probability $\tau$ such that if $P(S_1 > S_2) > \tau$, the strategy with the higher expected return is selected, and if $P(S_1 > S_2) \leq \tau$, the strategy with the lower volatility is chosen. This threshold ensures that the strategy selection is based on a balance between return and risk, incorporating the estimated probability of one asset outperforming the other.
\subsection{Position holding logic}
\subsection{Transaction cost considerations}

\section{Backtesting Framework}
Describe the backtesting process and how the strategies are evaluated.


